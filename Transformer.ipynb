{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Dense, Dropout, LayerNormalization, MultiHeadAttention, Layer\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import CodonAdaptationIndex\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "print(f\"Eager execution enabled: {tf.executing_eagerly()}\")\n",
    "print(f\"TensorFlow version: {tf._version_}\")\n",
    "\n",
    "def load_data(excel_path, sheet_name=0, max_rows=None):\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "        print(f\"Columns in the Excel file: {df.columns.tolist()}\")\n",
    "        print(df.head())\n",
    "        if 'Sequence' not in df.columns:\n",
    "            raise ValueError(\"Excel file must contain a 'Sequence' column.\")\n",
    "        df = df[['Sequence']].dropna()\n",
    "        if max_rows:\n",
    "            df = df.head(max_rows)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        print(f\"Loaded {len(df)} sequences from the Excel file.\")\n",
    "        df['Protein'] = df['Sequence'].apply(lambda x: str(Seq(x).translate(to_stop=True)))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def add_space_to_sequence(sequence):\n",
    "    return ' '.join([sequence[i:i+3] for i in range(0, len(sequence), 3)])\n",
    "\n",
    "def introduce_realistic_mutations(codons, mutation_rate=0.05, min_codon_length=3, max_attempts=10):\n",
    "    nucleotides = ['A', 'T', 'C', 'G']\n",
    "    attempts = 0\n",
    "    while True:\n",
    "        mutated_codons = []\n",
    "        for codon in codons:\n",
    "            mutated_codon = list(codon)\n",
    "            for i in range(len(mutated_codon)):\n",
    "                if random.random() < mutation_rate:\n",
    "                    mutated_codon[i] = random.choice([nuc for nuc in nucleotides if nuc != mutated_codon[i]])\n",
    "            if random.random() < mutation_rate:\n",
    "                insertion_index = random.randint(0, len(mutated_codon))\n",
    "                mutated_codon.insert(insertion_index, random.choice(nucleotides))\n",
    "            if random.random() < mutation_rate and len(mutated_codon) > 1:\n",
    "                deletion_index = random.randint(0, len(mutated_codon) - 1)\n",
    "                del mutated_codon[deletion_index]\n",
    "            mutated_codons.append(''.join(mutated_codon))\n",
    "        if len(mutated_codons) >= min_codon_length or attempts >= max_attempts:\n",
    "            break\n",
    "        attempts += 1\n",
    "    return mutated_codons\n",
    "\n",
    "def preprocess_sequences(df, mutation_rate=0.05, min_codon_length=3):\n",
    "    df['Sequence_with_spaces'] = df['Sequence'].apply(add_space_to_sequence)\n",
    "    codon_sequences = [seq.split(' ') for seq in df['Sequence_with_spaces'].values]\n",
    "    print(\"Introducing mutations to sequences...\")\n",
    "    bad_sequences = []\n",
    "    for idx, seq in enumerate(codon_sequences):\n",
    "        mutated_seq = introduce_realistic_mutations(seq, mutation_rate, min_codon_length)\n",
    "        bad_sequences.append(mutated_seq)\n",
    "        if (idx + 1) % 100 == 0 or (idx + 1) == len(codon_sequences):\n",
    "            print(f\"Processed {idx + 1}/{len(codon_sequences)} sequences.\")\n",
    "    flat_good_sequences = [' '.join(seq) for seq in codon_sequences]\n",
    "    flat_bad_sequences = [' '.join(seq) for seq in bad_sequences]\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=False, lower=False)\n",
    "    tokenizer.fit_on_texts(flat_good_sequences + flat_bad_sequences)\n",
    "    tokenized_good_sequences = tokenizer.texts_to_sequences(flat_good_sequences)\n",
    "    tokenized_bad_sequences = tokenizer.texts_to_sequences(flat_bad_sequences)\n",
    "    protein_sequences = df['Protein'].values\n",
    "    return tokenizer, tokenized_good_sequences, tokenized_bad_sequences, protein_sequences\n",
    "\n",
    "def filter_zero_length_sequences(tokenized_good, tokenized_bad, protein_sequences):\n",
    "    filtered_good = []\n",
    "    filtered_bad = []\n",
    "    filtered_protein = []\n",
    "    for idx, (good_seq, bad_seq, prot_seq) in enumerate(zip(tokenized_good, tokenized_bad, protein_sequences)):\n",
    "        if len(good_seq) > 0 and len(bad_seq) > 0:\n",
    "            filtered_good.append(good_seq)\n",
    "            filtered_bad.append(bad_seq)\n",
    "            filtered_protein.append(prot_seq)\n",
    "    print(f\"Filtered to {len(filtered_good)} sequences after removing zero-length sequences.\")\n",
    "    return filtered_good, filtered_bad, filtered_protein\n",
    "\n",
    "def determine_max_sequence_length(tokenized_sequences, percentile=95):\n",
    "    sequence_lengths = [len(seq) for seq in tokenized_sequences]\n",
    "    max_sequence_length = int(np.percentile(sequence_lengths, percentile))\n",
    "    print(f\"Determined max_sequence_length as the {percentile}th percentile: {max_sequence_length}\")\n",
    "    return max_sequence_length\n",
    "\n",
    "def pad_sequences_custom(tokenized_sequences, max_sequence_length):\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_sequences, maxlen=max_sequence_length, padding='post', truncating='post', dtype='int32')\n",
    "\n",
    "def get_protein_embeddings(protein_sequences, esm_model, esm_tokenizer, embeddings_cache_path=None):\n",
    "    if embeddings_cache_path and os.path.exists(embeddings_cache_path):\n",
    "        print(\"Loading cached protein embeddings...\")\n",
    "        embeddings = np.load(embeddings_cache_path)\n",
    "        return embeddings\n",
    "    print(\"Generating ESM-2 embeddings for protein sequences...\")\n",
    "    esm_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        esm_model.to('cuda')\n",
    "        print(\"Using GPU for ESM-2 embeddings.\")\n",
    "    else:\n",
    "        esm_model.to('cpu')\n",
    "        print(\"Using CPU for ESM-2 embeddings.\")\n",
    "    embeddings = []\n",
    "    for idx, seq in enumerate(protein_sequences):\n",
    "        esm_max_length = 1022\n",
    "        inputs = esm_tokenizer(\n",
    "            seq,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=esm_max_length\n",
    "        )\n",
    "        device = next(esm_model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = esm_model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            masked_hidden_states = hidden_states * attention_mask\n",
    "            sum_hidden = masked_hidden_states.sum(dim=1)\n",
    "            total_tokens = attention_mask.sum(dim=1)\n",
    "            embedding = (sum_hidden / total_tokens).squeeze().cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "        if (idx + 1) % 100 == 0 or (idx + 1) == len(protein_sequences):\n",
    "            print(f\"Processed {idx + 1}/{len(protein_sequences)} protein sequences.\")\n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "    if embeddings_cache_path:\n",
    "        np.save(embeddings_cache_path, embeddings)\n",
    "    return embeddings\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def _init_(self, embed_dim=128, num_heads=4, ff_dim=512, rate=0.1, **kwargs):\n",
    "        super(TransformerBlock, self)._init_(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "        self.supports_masking = True\n",
    "        self.att = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [Dense(self.ff_dim, activation='relu'), Dense(self.embed_dim)]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(self.rate)\n",
    "        self.dropout2 = Dropout(self.rate)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        attn_output = self.att(inputs, inputs, inputs, training=training, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(TransformerBlock, self).get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'ff_dim': self.ff_dim,\n",
    "            'rate': self.rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def create_model(vocab_size, max_sequence_length, protein_embedding_dim):\n",
    "    decoder_embed_dim = 128\n",
    "    num_heads = 4\n",
    "    ff_dim = 512\n",
    "    num_layers = 2\n",
    "    dropout_rate = 0.1\n",
    "    protein_inputs = Input(shape=(protein_embedding_dim,), name=\"protein_embeddings\")\n",
    "    protein_projection = Dense(decoder_embed_dim, activation='relu', name='protein_projection')(protein_inputs)\n",
    "    repeated_protein_embeddings = tf.keras.layers.RepeatVector(max_sequence_length)(protein_projection)\n",
    "    decoder_inputs = Input(shape=(max_sequence_length,), name=\"decoder_inputs\")\n",
    "    decoder_embedding = Embedding(input_dim=vocab_size, output_dim=decoder_embed_dim, mask_zero=True)(decoder_inputs)\n",
    "    positions = tf.range(start=0, limit=max_sequence_length, delta=1)\n",
    "    position_embeddings = Embedding(input_dim=max_sequence_length, output_dim=decoder_embed_dim)(positions)\n",
    "    x = decoder_embedding + position_embeddings + repeated_protein_embeddings\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim=decoder_embed_dim, num_heads=num_heads, ff_dim=ff_dim, rate=dropout_rate)(x)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "    model = Model(inputs=[protein_inputs, decoder_inputs], outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "import heapq\n",
    "\n",
    "def beam_search_decoder(predictions, beam_width=3):\n",
    "    sequences = [([], 0.0)]\n",
    "    for row in predictions[0]:\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            for j in range(len(row)):\n",
    "                candidate = (seq + [j], score - np.log(row[j] + 1e-8))\n",
    "                all_candidates.append(candidate)\n",
    "        sequences = heapq.nsmallest(beam_width, all_candidates, key=lambda tup: tup[1])\n",
    "    return sequences\n",
    "\n",
    "def correct_sequence_gui(input_text_widget, output_text_widget, tokenizer, model, max_sequence_length, index_to_codon, esm_tokenizer, esm_model):\n",
    "    user_input = input_text_widget.get(\"1.0\", tk.END).strip().upper()\n",
    "    if not user_input:\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a protein sequence to optimize.\")\n",
    "        return\n",
    "    if not re.fullmatch(r'[A-Z]+', user_input):\n",
    "        messagebox.showwarning(\"Input Error\", \"Invalid sequence. Please enter a valid protein sequence.\")\n",
    "        return\n",
    "    try:\n",
    "        esm_max_length = 1022\n",
    "        device = next(esm_model.parameters()).device\n",
    "        inputs = esm_tokenizer(\n",
    "            user_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=esm_max_length\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = esm_model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            masked_hidden_states = hidden_states * attention_mask\n",
    "            sum_hidden = masked_hidden_states.sum(dim=1)\n",
    "            total_tokens = attention_mask.sum(dim=1)\n",
    "            protein_embedding = (sum_hidden / total_tokens).squeeze().cpu().numpy()\n",
    "        decoder_input = [0]\n",
    "        output_tokens = []\n",
    "        for _ in range(max_sequence_length):\n",
    "            decoder_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                [decoder_input], maxlen=max_sequence_length, padding='post'\n",
    "            )\n",
    "            predicted_probs = model.predict([protein_embedding.reshape(1, -1), decoder_input_padded], verbose=0)\n",
    "            next_token_probs = predicted_probs[0, len(decoder_input)-1]\n",
    "            next_token = np.argmax(next_token_probs)\n",
    "            if next_token == 0:\n",
    "                break\n",
    "            output_tokens.append(next_token)\n",
    "            decoder_input.append(next_token)\n",
    "        predicted_codon_sequence = []\n",
    "        for token in output_tokens:\n",
    "            codon = index_to_codon.get(token, '')\n",
    "            if codon:\n",
    "                predicted_codon_sequence.append(codon)\n",
    "        optimized_sequence = ''.join(predicted_codon_sequence)\n",
    "        cai_calculator = CodonAdaptationIndex()\n",
    "        cai_value = cai_calculator.cai_for_gene(optimized_sequence)\n",
    "        output_text_widget.delete(\"1.0\", tk.END)\n",
    "        output_text_widget.insert(tk.END, f\"Optimized DNA Sequence:\\n{optimized_sequence}\\n\\nCAI: {cai_value:.4f}\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Processing Error\", f\"An error occurred during processing:\\n{e}\")\n",
    "\n",
    "def sample_bad_sequence_gui(input_text_widget, flat_protein_sequences):\n",
    "    if not flat_protein_sequences:\n",
    "        messagebox.showwarning(\"No Data\", \"No protein sequences available to sample.\")\n",
    "        return\n",
    "    random_index = random.randint(0, len(flat_protein_sequences) - 1)\n",
    "    sample_sequence = flat_protein_sequences[random_index]\n",
    "    input_text_widget.delete(\"1.0\", tk.END)\n",
    "    input_text_widget.insert(tk.END, sample_sequence + \"\\n\")\n",
    "\n",
    "def create_gui(tokenizer, model, max_sequence_length, index_to_codon, flat_protein_sequences, esm_tokenizer, esm_model):\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Protein to Optimized DNA Sequence App\")\n",
    "    window.geometry('800x600')\n",
    "    input_label = tk.Label(window, text=\"Enter a protein sequence:\")\n",
    "    input_label.pack(pady=(10, 0))\n",
    "    input_text_widget = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=80, height=10)\n",
    "    input_text_widget.pack(pady=(5, 10))\n",
    "    correct_button = tk.Button(\n",
    "        window,\n",
    "        text=\"Generate Optimized DNA Sequence\",\n",
    "        command=lambda: correct_sequence_gui(\n",
    "            input_text_widget, output_text_widget, tokenizer, model, max_sequence_length, index_to_codon, esm_tokenizer, esm_model\n",
    "        )\n",
    "    )\n",
    "    correct_button.pack(pady=10)\n",
    "    sample_button = tk.Button(\n",
    "        window,\n",
    "        text=\"Generate Sample Protein Sequence\",\n",
    "        command=lambda: sample_bad_sequence_gui(\n",
    "            input_text_widget, flat_protein_sequences\n",
    "        )\n",
    "    )\n",
    "    sample_button.pack(pady=5)\n",
    "    output_label = tk.Label(window, text=\"Output:\")\n",
    "    output_label.pack(pady=(20, 0))\n",
    "    output_text_widget = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=80, height=15)\n",
    "    output_text_widget.pack(pady=(5, 10))\n",
    "    window.mainloop()\n",
    "\n",
    "def main():\n",
    "    excel_path = r\"/Users/omar/Desktop/Database 2.xlsx\"\n",
    "    tokenizer_path = 'toke1nizer_gene4.pkl'\n",
    "    tokenized_good_sequences_path = 'tokenized_3good_sequences2.pkl'\n",
    "    tokenized_bad_sequences_path = 'tokeni24zed_bad_sequences2.pkl'\n",
    "    protein_embeddings_path = 'protein_emb3e24ddings2.npy'\n",
    "    max_sequence_length_path = 'max_se24quence_leng34th2.npy'\n",
    "    model_path = 'trained_mo2344del_gene2.keras'\n",
    "    print(\"Loading ESM-2 model and tokenizer...\")\n",
    "    esm_model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "    esm_tokenizer = AutoTokenizer.from_pretrained(esm_model_name)\n",
    "    esm_model = AutoModel.from_pretrained(esm_model_name)\n",
    "    if (os.path.exists(tokenizer_path) and \n",
    "        os.path.exists(tokenized_good_sequences_path) and \n",
    "        os.path.exists(tokenized_bad_sequences_path) and \n",
    "        os.path.exists(protein_embeddings_path)):\n",
    "        print(\"Loading preprocessed data...\")\n",
    "        with open(tokenizer_path, 'rb') as handle:\n",
    "            tokenizer = pickle.load(handle)\n",
    "        with open(tokenized_good_sequences_path, 'rb') as f:\n",
    "            tokenized_good_sequences = pickle.load(f)\n",
    "        with open(tokenized_bad_sequences_path, 'rb') as f:\n",
    "            tokenized_bad_sequences = pickle.load(f)\n",
    "        protein_embeddings = np.load(protein_embeddings_path)\n",
    "        df = load_data(excel_path, sheet_name=0)\n",
    "    else:\n",
    "        df = load_data(excel_path, sheet_name=0)\n",
    "        tokenizer, tokenized_good_sequences, tokenized_bad_sequences, protein_sequences = preprocess_sequences(\n",
    "            df, mutation_rate=0.05, min_codon_length=3\n",
    "        )\n",
    "        tokenized_good_sequences, tokenized_bad_sequences, protein_sequences = filter_zero_length_sequences(\n",
    "            tokenized_good_sequences, tokenized_bad_sequences, protein_sequences\n",
    "        )\n",
    "        with open(tokenizer_path, 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(tokenized_good_sequences_path, 'wb') as f:\n",
    "            pickle.dump(tokenized_good_sequences, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(tokenized_bad_sequences_path, 'wb') as f:\n",
    "            pickle.dump(tokenized_bad_sequences, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        protein_embeddings = get_protein_embeddings(protein_sequences, esm_model, esm_tokenizer, protein_embeddings_path)\n",
    "    if os.path.exists(max_sequence_length_path):\n",
    "        max_sequence_length = int(np.load(max_sequence_length_path))\n",
    "    else:\n",
    "        max_sequence_length = determine_max_sequence_length(\n",
    "            tokenized_good_sequences + tokenized_bad_sequences, percentile=95\n",
    "        )\n",
    "        np.save(max_sequence_length_path, max_sequence_length)\n",
    "    print(\"Padding sequences...\")\n",
    "    padded_good_sequences = pad_sequences_custom(tokenized_good_sequences, max_sequence_length)\n",
    "    padded_bad_sequences = pad_sequences_custom(tokenized_bad_sequences, max_sequence_length)\n",
    "    print(\"Preparing datasets...\")\n",
    "    X_train, X_val, y_train, y_val, prot_train, prot_val = train_test_split(\n",
    "        padded_bad_sequences, padded_good_sequences, protein_embeddings, test_size=0.2, random_state=42\n",
    "    )\n",
    "    y_train = y_train.astype(np.int32)\n",
    "    y_val = y_val.astype(np.int32)\n",
    "    batch_size = 32\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((prot_train, X_train), y_train))\n",
    "    train_dataset = train_dataset.cache().shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((prot_val, X_val), y_val))\n",
    "    val_dataset = val_dataset.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading the trained model...\")\n",
    "        custom_objects = {'TransformerBlock': TransformerBlock}\n",
    "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "    else:\n",
    "        print(\"Creating and compiling the model...\")\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "        protein_embedding_dim = protein_embeddings.shape[1]\n",
    "        model = create_model(vocab_size, max_sequence_length, protein_embedding_dim)\n",
    "        model.summary()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=False)\n",
    "        print(\"Starting training...\")\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=5,\n",
    "            callbacks=[early_stopping, reduce_lr, tensorboard_callback],\n",
    "            verbose=1\n",
    "        )\n",
    "        model.save(model_path)\n",
    "        print(\"Evaluating the model on validation data...\")\n",
    "        test_loss, test_accuracy = model.evaluate(val_dataset)\n",
    "        print(f\"Validation Loss: {test_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {test_accuracy:.4f}\")\n",
    "    index_to_codon = {v: k for k, v in tokenizer.word_index.items()}\n",
    "    flat_protein_sequences = df['Protein'].tolist()\n",
    "    esm_model.eval()\n",
    "    if torch.cuda.is_available():\n",
    "        esm_model.to('cuda')\n",
    "    else:\n",
    "        esm_model.to('cpu')\n",
    "    print(\"Launching the GUI...\")\n",
    "    create_gui(tokenizer, model, max_sequence_length, index_to_codon, flat_protein_sequences, esm_tokenizer, esm_model)\n",
    "\n",
    "if _name_ == '_main_':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== IMPORT LIBRARIES ====================\n",
    "# Core data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning framework\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Dense, Dropout, \n",
    "    LayerNormalization, MultiHeadAttention, Layer\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "# Data splitting and bioinformatics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqUtils import CodonAdaptationIndex\n",
    "\n",
    "# GUI components\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext, messagebox\n",
    "\n",
    "# Protein language model\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Enable eager execution for debugging\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# ==================== DATA LOADING FUNCTIONS ====================\n",
    "def load_data(excel_path, sheet_name=0, max_rows=None):\n",
    "    \"\"\"\n",
    "    Load and preprocess DNA sequence data from Excel file\n",
    "    Args:\n",
    "        excel_path: Path to Excel file containing DNA sequences\n",
    "        sheet_name: Sheet name/number to read\n",
    "        max_rows: Maximum number of rows to load\n",
    "    Returns:\n",
    "        DataFrame with processed sequences and protein translations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read Excel file and validate structure\n",
    "        df = pd.read_excel(excel_path, sheet_name=sheet_name)\n",
    "        if 'Sequence' not in df.columns:\n",
    "            raise ValueError(\"Excel file must contain a 'Sequence' column.\")\n",
    "        \n",
    "        # Clean and process sequences\n",
    "        df = df[['Sequence']].dropna()\n",
    "        if max_rows:\n",
    "            df = df.head(max_rows)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Translate DNA to protein sequences\n",
    "        df['Protein'] = df['Sequence'].apply(\n",
    "            lambda x: str(Seq(x).translate(to_stop=True))\n",
    "        )\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# ==================== SEQUENCE PROCESSING FUNCTIONS ====================\n",
    "def add_space_to_sequence(sequence):\n",
    "    \"\"\"Convert DNA sequence to space-separated codon string\"\"\"\n",
    "    return ' '.join([sequence[i:i+3] for i in range(0, len(sequence), 3)])\n",
    "\n",
    "def introduce_realistic_mutations(codons, mutation_rate=0.05, min_codon_length=3, max_attempts=10):\n",
    "    \"\"\"\n",
    "    Generate realistic DNA mutations for training data augmentation\n",
    "    Implements three types of mutations:\n",
    "    1. Substitutions: Replace nucleotides in codons\n",
    "    2. Insertions: Add random nucleotides\n",
    "    3. Deletions: Remove nucleotides\n",
    "    \"\"\"\n",
    "    nucleotides = ['A', 'T', 'C', 'G']\n",
    "    attempts = 0\n",
    "    while True:\n",
    "        mutated_codons = []\n",
    "        for codon in codons:\n",
    "            # Create mutable copy of codon\n",
    "            mutated_codon = list(codon)\n",
    "            \n",
    "            # Apply substitutions\n",
    "            for i in range(len(mutated_codon)):\n",
    "                if random.random() < mutation_rate:\n",
    "                    mutated_codon[i] = random.choice(\n",
    "                        [nuc for nuc in nucleotides if nuc != mutated_codon[i]]\n",
    "                    )\n",
    "            \n",
    "            # Apply insertions\n",
    "            if random.random() < mutation_rate:\n",
    "                insertion_index = random.randint(0, len(mutated_codon))\n",
    "                mutated_codon.insert(insertion_index, random.choice(nucleotides))\n",
    "            \n",
    "            # Apply deletions\n",
    "            if random.random() < mutation_rate and len(mutated_codon) > 1:\n",
    "                deletion_index = random.randint(0, len(mutated_codon) - 1)\n",
    "                del mutated_codon[deletion_index]\n",
    "            \n",
    "            mutated_codons.append(''.join(mutated_codon))\n",
    "        \n",
    "        # Validate minimum length requirement\n",
    "        if len(mutated_codons) >= min_codon_length or attempts >= max_attempts:\n",
    "            break\n",
    "        attempts += 1\n",
    "    return mutated_codons\n",
    "\n",
    "# ==================== DATA PREPROCESSING PIPELINE ====================\n",
    "def preprocess_sequences(df, mutation_rate=0.05, min_codon_length=3):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline:\n",
    "    1. Split sequences into codons\n",
    "    2. Generate mutated versions for training\n",
    "    3. Tokenize sequences\n",
    "    \"\"\"\n",
    "    # Convert sequences to codon lists\n",
    "    df['Sequence_with_spaces'] = df['Sequence'].apply(add_space_to_sequence)\n",
    "    codon_sequences = [seq.split(' ') for seq in df['Sequence_with_spaces'].values]\n",
    "    \n",
    "    # Generate mutated sequences\n",
    "    print(\"Introducing mutations to sequences...\")\n",
    "    bad_sequences = []\n",
    "    for idx, seq in enumerate(codon_sequences):\n",
    "        mutated_seq = introduce_realistic_mutations(seq, mutation_rate, min_codon_length)\n",
    "        bad_sequences.append(mutated_seq)\n",
    "        # Progress tracking\n",
    "        if (idx + 1) % 100 == 0 or (idx + 1) == len(codon_sequences):\n",
    "            print(f\"Processed {idx + 1}/{len(codon_sequences)} sequences.\")\n",
    "    \n",
    "    # Create tokenizer and process sequences\n",
    "    flat_good_sequences = [' '.join(seq) for seq in codon_sequences]\n",
    "    flat_bad_sequences = [' '.join(seq) for seq in bad_sequences]\n",
    "    \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=False, lower=False)\n",
    "    tokenizer.fit_on_texts(flat_good_sequences + flat_bad_sequences)\n",
    "    \n",
    "    return (\n",
    "        tokenizer,\n",
    "        tokenizer.texts_to_sequences(flat_good_sequences),\n",
    "        tokenizer.texts_to_sequences(flat_bad_sequences),\n",
    "        df['Protein'].values\n",
    "    )\n",
    "\n",
    "# ==================== SEQUENCE FILTERING AND PADDING ====================\n",
    "def filter_zero_length_sequences(tokenized_good, tokenized_bad, protein_sequences):\n",
    "    \"\"\"Remove sequences with zero length after tokenization\"\"\"\n",
    "    filtered_good = []\n",
    "    filtered_bad = []\n",
    "    filtered_protein = []\n",
    "    for good_seq, bad_seq, prot_seq in zip(tokenized_good, tokenized_bad, protein_sequences):\n",
    "        if len(good_seq) > 0 and len(bad_seq) > 0:\n",
    "            filtered_good.append(good_seq)\n",
    "            filtered_bad.append(bad_seq)\n",
    "            filtered_protein.append(prot_seq)\n",
    "    print(f\"Filtered to {len(filtered_good)} valid sequences.\")\n",
    "    return filtered_good, filtered_bad, filtered_protein\n",
    "\n",
    "def determine_max_sequence_length(tokenized_sequences, percentile=95):\n",
    "    \"\"\"Calculate sequence length for padding based on percentile\"\"\"\n",
    "    sequence_lengths = [len(seq) for seq in tokenized_sequences]\n",
    "    return int(np.percentile(sequence_lengths, percentile))\n",
    "\n",
    "def pad_sequences_custom(tokenized_sequences, max_sequence_length):\n",
    "    \"\"\"Custom sequence padding with post-truncation\"\"\"\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_sequences, \n",
    "        maxlen=max_sequence_length, \n",
    "        padding='post', \n",
    "        truncating='post', \n",
    "        dtype='int32'\n",
    "    )\n",
    "\n",
    "# ==================== PROTEIN EMBEDDING GENERATION ====================\n",
    "def get_protein_embeddings(protein_sequences, esm_model, esm_tokenizer, embeddings_cache_path=None):\n",
    "    \"\"\"\n",
    "    Generate protein embeddings using ESM-2 model\n",
    "    Implements caching to avoid recomputation\n",
    "    \"\"\"\n",
    "    if embeddings_cache_path and os.path.exists(embeddings_cache_path):\n",
    "        return np.load(embeddings_cache_path)\n",
    "    \n",
    "    print(\"Generating ESM-2 embeddings...\")\n",
    "    esm_model.eval()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    esm_model.to(device)\n",
    "    \n",
    "    embeddings = []\n",
    "    for idx, seq in enumerate(protein_sequences):\n",
    "        # Tokenize and process protein sequence\n",
    "        inputs = esm_tokenizer(\n",
    "            seq,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=1022\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = esm_model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "            # Masked mean pooling\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            embedding = (hidden_states * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "            \n",
    "        embeddings.append(embedding.cpu().numpy().squeeze())\n",
    "        \n",
    "        # Progress tracking\n",
    "        if (idx + 1) % 100 == 0 or (idx + 1) == len(protein_sequences):\n",
    "            print(f\"Processed {idx + 1}/{len(protein_sequences)} proteins.\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    if embeddings_cache_path:\n",
    "        np.save(embeddings_cache_path, embeddings)\n",
    "    return embeddings\n",
    "\n",
    "# ==================== TRANSFORMER MODEL ARCHITECTURE ====================\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom transformer block implementation\"\"\"\n",
    "    def __init__(self, embed_dim=128, num_heads=4, ff_dim=512, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu'), \n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        # Self-attention mechanism\n",
    "        attn_output = self.att(inputs, inputs, attention_mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def create_model(vocab_size, max_sequence_length, protein_embedding_dim):\n",
    "    \"\"\"\n",
    "    Build the complete transformer model architecture\n",
    "    Inputs:\n",
    "    - Protein embeddings (from ESM-2)\n",
    "    - DNA sequence tokens\n",
    "    Output:\n",
    "    - Probability distribution over vocabulary\n",
    "    \"\"\"\n",
    "    # Input layers\n",
    "    protein_inputs = Input(shape=(protein_embedding_dim,), name=\"protein_embeddings\")\n",
    "    decoder_inputs = Input(shape=(max_sequence_length,), name=\"decoder_inputs\")\n",
    "    \n",
    "    # Protein embedding processing\n",
    "    protein_projection = Dense(128, activation='relu')(protein_inputs)\n",
    "    repeated_protein = tf.keras.layers.RepeatVector(max_sequence_length)(protein_projection)\n",
    "    \n",
    "    # DNA sequence processing\n",
    "    decoder_embedding = Embedding(vocab_size, 128, mask_zero=True)(decoder_inputs)\n",
    "    positions = tf.range(start=0, limit=max_sequence_length, delta=1)\n",
    "    position_embedding = Embedding(max_sequence_length, 128)(positions)\n",
    "    \n",
    "    # Combine embeddings\n",
    "    x = decoder_embedding + position_embedding + repeated_protein\n",
    "    \n",
    "    # Transformer layers\n",
    "    for _ in range(2):\n",
    "        x = TransformerBlock()(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs=[protein_inputs, decoder_inputs], outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005, clipnorm=1.0),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ==================== SEQUENCE GENERATION UTILITIES ====================\n",
    "def beam_search_decoder(predictions, beam_width=3):\n",
    "    \"\"\"Alternative decoding strategy (not used in current implementation)\"\"\"\n",
    "    sequences = [([], 0.0)]\n",
    "    for row in predictions[0]:\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            for j in range(len(row)):\n",
    "                candidate = (seq + [j], score - np.log(row[j] + 1e-8))\n",
    "                all_candidates.append(candidate)\n",
    "        sequences = heapq.nsmallest(beam_width, all_candidates, key=lambda tup: tup[1])\n",
    "    return sequences\n",
    "\n",
    "# ==================== GUI COMPONENTS ====================\n",
    "def correct_sequence_gui(input_text_widget, output_text_widget, tokenizer, model, \n",
    "                        max_sequence_length, index_to_codon, esm_tokenizer, esm_model):\n",
    "    \"\"\"Main sequence optimization handler for GUI\"\"\"\n",
    "    # Validate input\n",
    "    user_input = input_text_widget.get(\"1.0\", tk.END).strip().upper()\n",
    "    if not user_input:\n",
    "        messagebox.showwarning(\"Input Error\", \"Please enter a protein sequence.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Generate protein embedding\n",
    "        inputs = esm_tokenizer(\n",
    "            user_input,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=1022\n",
    "        ).to(esm_model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = esm_model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "            # Pooling\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            protein_embedding = (hidden_states * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "            protein_embedding = protein_embedding.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Generate DNA sequence\n",
    "        decoder_input = [0]\n",
    "        output_tokens = []\n",
    "        for _ in range(max_sequence_length):\n",
    "            decoder_input_padded = pad_sequences_custom([decoder_input], max_sequence_length)\n",
    "            pred_probs = model.predict(\n",
    "                [protein_embedding.reshape(1, -1), decoder_input_padded], \n",
    "                verbose=0\n",
    "            )\n",
    "            next_token = np.argmax(pred_probs[0, len(decoder_input)-1])\n",
    "            if next_token == 0:  # Stop token\n",
    "                break\n",
    "            output_tokens.append(next_token)\n",
    "            decoder_input.append(next_token)\n",
    "        \n",
    "        # Convert tokens to codons\n",
    "        optimized_sequence = ''.join(\n",
    "            [index_to_codon.get(token, '') for token in output_tokens]\n",
    "        )\n",
    "        \n",
    "        # Calculate CAI\n",
    "        cai = CodonAdaptationIndex().cai_for_gene(optimized_sequence)\n",
    "        \n",
    "        # Update GUI\n",
    "        output_text_widget.delete(\"1.0\", tk.END)\n",
    "        output_text_widget.insert(tk.END, \n",
    "            f\"Optimized DNA Sequence:\\n{optimized_sequence}\\n\\nCAI: {cai:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", f\"Processing failed:\\n{e}\")\n",
    "\n",
    "def create_gui(tokenizer, model, max_sequence_length, index_to_codon, \n",
    "             flat_protein_sequences, esm_tokenizer, esm_model):\n",
    "    \"\"\"Main GUI window setup\"\"\"\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Protein Sequence Optimizer\")\n",
    "    window.geometry('800x600')\n",
    "    \n",
    "    # Input components\n",
    "    input_label = tk.Label(window, text=\"Enter protein sequence:\")\n",
    "    input_label.pack(pady=10)\n",
    "    input_text = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=80, height=10)\n",
    "    input_text.pack()\n",
    "    \n",
    "    # Control buttons\n",
    "    button_frame = tk.Frame(window)\n",
    "    button_frame.pack(pady=10)\n",
    "    \n",
    "    optimize_btn = tk.Button(\n",
    "        button_frame,\n",
    "        text=\"Optimize DNA\",\n",
    "        command=lambda: correct_sequence_gui(\n",
    "            input_text, output_text, tokenizer, model,\n",
    "            max_sequence_length, index_to_codon, esm_tokenizer, esm_model\n",
    "        )\n",
    "    )\n",
    "    optimize_btn.pack(side=tk.LEFT, padx=5)\n",
    "    \n",
    "    sample_btn = tk.Button(\n",
    "        button_frame,\n",
    "        text=\"Sample Protein\",\n",
    "        command=lambda: sample_bad_sequence_gui(input_text, flat_protein_sequences)\n",
    "    )\n",
    "    sample_btn.pack(side=tk.LEFT, padx=5)\n",
    "    \n",
    "    # Output components\n",
    "    output_label = tk.Label(window, text=\"Optimization Result:\")\n",
    "    output_label.pack(pady=10)\n",
    "    output_text = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=80, height=15)\n",
    "    output_text.pack()\n",
    "    \n",
    "    window.mainloop()\n",
    "\n",
    "# ==================== MAIN EXECUTION PIPELINE ====================\n",
    "def main():\n",
    "    \"\"\"End-to-end execution pipeline\"\"\"\n",
    "    # Configuration\n",
    "    PATHS = {\n",
    "        'excel': \"/path/to/sequences.xlsx\",\n",
    "        'tokenizer': 'tokenizer.pkl',\n",
    "        'model': 'optimizer_model.keras'\n",
    "    }\n",
    "    \n",
    "    # Load ESM-2 model\n",
    "    esm_model = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "    esm_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "    \n",
    "    # Data processing\n",
    "    if not all(os.path.exists(p) for p in PATHS.values()):\n",
    "        df = load_data(PATHS['excel'])\n",
    "        tokenizer, good_seqs, bad_seqs, proteins = preprocess_sequences(df)\n",
    "        # ... [rest of processing]\n",
    "    \n",
    "    # Model training/loading\n",
    "    if os.path.exists(PATHS['model']):\n",
    "        model = tf.keras.models.load_model(PATHS['model'])\n",
    "    else:\n",
    "        model = create_model(...)\n",
    "        # ... [training code]\n",
    "    \n",
    "    # Launch GUI\n",
    "    create_gui(tokenizer, model, max_seq_len, index_to_codon, proteins, esm_tokenizer, esm_model)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
